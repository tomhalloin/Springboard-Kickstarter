{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:11:11.562628Z",
     "start_time": "2019-06-27T18:11:11.555647Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this exercise, I will walk through this article linked below: \n",
    "# https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:52:36.689675Z",
     "start_time": "2019-06-27T21:52:36.674716Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Uploading pandas dataframe\n",
    "import re, string, unicodedata # Removing punctuation, converting rows into string\n",
    "import nltk # Natural language processing library\n",
    "import contractions # Parses contractions\n",
    "import inflect # Generating plurals, singulars, numbers to words\n",
    "from bs4 import BeautifulSoup # online scraper\n",
    "from nltk import word_tokenize, sent_tokenize #Tokenization of words, sentences\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer #Lemmatization of words\n",
    "from nltk import punkt # Needed to tokenize words\n",
    "from nltk.corpus import stopwords # Needed for normalization\n",
    "from nltk.corpus import wordnet # Find meaning of words, synonyms, antonyms.\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Gets counts of words for sparse matrix\n",
    "from scipy import sparse # Saves sparse matrix for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:38:54.713326Z",
     "start_time": "2019-06-27T21:38:50.844960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in kickstarter data as csv\n",
    "\n",
    "kickstarter = pd.read_csv(r'kickstarter.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:12:12.091573Z",
     "start_time": "2019-06-27T18:12:11.758234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Operating on the text columns, name, and blurb.\n",
    "\n",
    "text = kickstarter_text['name'] + ' ' + kickstarter_text['blurb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:12:15.489795Z",
     "start_time": "2019-06-27T18:12:15.441924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting everything as a string\n",
    "\n",
    "to_string = text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T19:04:30.186465Z",
     "start_time": "2019-06-27T19:04:30.171165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing contractions.\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:14:38.343817Z",
     "start_time": "2019-06-27T18:12:36.825669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing contractions\n",
    "\n",
    "without_contractions = to_string.map(lambda x: replace_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:17:29.820564Z",
     "start_time": "2019-06-27T18:15:37.009829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying word tokenization\n",
    "\n",
    "tokens = without_contractions.map(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:21:54.460327Z",
     "start_time": "2019-06-27T21:21:54.395500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# Taking out replacing numbers because not important for analysis.\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T19:04:30.171165Z",
     "start_time": "2019-06-27T18:18:44.845851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalizing words using the functions below.\n",
    "\n",
    "normalized = tokens.map(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:17:19.682551Z",
     "start_time": "2019-06-27T21:17:19.634687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizing verbs: changing verbs to infinite form \n",
    "\n",
    "lemmatized = normalized.map(lambda x: lemmatize_verbs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T19:25:20.624357Z",
     "start_time": "2019-06-27T19:25:18.038093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the lemmatized version to a CSV to read in later.\n",
    "\n",
    "lemmatized.to_csv('lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:49:16.124040Z",
     "start_time": "2019-06-27T20:49:15.163532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reads in csv from above to continue processing.\n",
    "\n",
    "df = pd.read_csv('lemmatized.csv', names=['list of words'], header=None)\n",
    "lemmatized = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:49:20.930123Z",
     "start_time": "2019-06-27T20:49:20.898215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list of words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['remix', 'explore', 'paint', 'place', 'digita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['photo', 'grant', 'hazel', 'eat', 'cake', 'ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['minecraft', 'digital', 'artanimations', 'nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['sacramento', 'nature', 'photograph', 'series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['north', 'africa', 'art', 'project', 'dream',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       list of words\n",
       "1  ['remix', 'explore', 'paint', 'place', 'digita...\n",
       "2  ['photo', 'grant', 'hazel', 'eat', 'cake', 'ba...\n",
       "3  ['minecraft', 'digital', 'artanimations', 'nee...\n",
       "4  ['sacramento', 'nature', 'photograph', 'series...\n",
       "5  ['north', 'africa', 'art', 'project', 'dream',..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only frequent words to remove product names and other specific words\n",
    "\n",
    "lemmatized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:14:53.793239Z",
     "start_time": "2019-06-27T21:14:50.317261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# Gets rid of numbers\n",
    "\n",
    "lemmatized['no_numbers'] = lemmatized['list of words'].apply(lambda x: re.sub('(\\d)+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:53:06.864682Z",
     "start_time": "2019-06-27T21:52:58.949061Z"
    }
   },
   "outputs": [],
   "source": [
    "#Gets rid of rare words\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=.001) \n",
    "X = vectorizer.fit_transform(lemmatized['no_numbers'])\n",
    "y = kickstarter['binary_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:44:27.915646Z",
     "start_time": "2019-06-27T21:44:25.019268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saves matrix for machine learning in other notebook. Comment is to read in matrix.\n",
    "\n",
    "sparse.save_npz(\"kickstarter.npz\", X)\n",
    "# your_matrix_back = sparse.load_npz(\"kickstarter.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T21:54:47.702987Z",
     "start_time": "2019-06-27T21:54:47.682043Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "vocabulary not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-248bbcc20129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    645\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: vocabulary not found"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
