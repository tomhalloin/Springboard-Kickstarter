{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes the text features (blurb, description) of the Kickstarter data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:10.991999Z",
     "start_time": "2019-08-08T23:42:10.985068Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this notebook, I will walk through this article linked below: \n",
    "# https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:11.007956Z",
     "start_time": "2019-08-08T23:42:10.996985Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Numpy is numpy\n",
    "import pandas as pd # Uploading pandas dataframe\n",
    "import re, string, unicodedata # Removing punctuation, converting rows into string\n",
    "import nltk # Natural language processing library\n",
    "import contractions # Parses contractions\n",
    "import inflect # Generating plurals, singulars, numbers to words\n",
    "from bs4 import BeautifulSoup # online scraper\n",
    "from nltk import word_tokenize, sent_tokenize #Tokenization of words, sentences\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer #Lemmatization of words\n",
    "from nltk import punkt # Needed to tokenize words\n",
    "from nltk.corpus import stopwords # Needed for normalization\n",
    "from nltk.corpus import wordnet # Find meaning of words, synonyms, antonyms.\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Gets counts of words for sparse matrix\n",
    "from scipy import sparse # Saves sparse matrix for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:14.958898Z",
     "start_time": "2019-08-08T23:42:11.012944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in kickstarter data as csv\n",
    "\n",
    "kickstarter = pd.read_csv(r'kickstarter.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:15.264408Z",
     "start_time": "2019-08-08T23:42:14.958898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Operating on the text columns, name, and blurb.\n",
    "\n",
    "text = kickstarter['name'] + ' ' + kickstarter['blurb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:15.311247Z",
     "start_time": "2019-08-08T23:42:15.266365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting everything as a string\n",
    "\n",
    "to_string = text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:42:15.323218Z",
     "start_time": "2019-08-08T23:42:15.314237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing contractions.\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:44:29.434843Z",
     "start_time": "2019-08-08T23:42:15.325208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing contractions\n",
    "\n",
    "without_contractions = to_string.map(lambda x: replace_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:46:46.589122Z",
     "start_time": "2019-08-08T23:44:29.434843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying word tokenization\n",
    "\n",
    "tokens = without_contractions.map(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:46:46.614756Z",
     "start_time": "2019-08-08T23:46:46.589122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not any(ch.isdigit() for ch in word):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# Moving stopwords_list out of function leads to massive performance improvement.\n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:47:38.642812Z",
     "start_time": "2019-08-08T23:46:46.614756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalizing words using the functions above. Stemming words provides a different analysis, I found the stemming takes\n",
    "# out too much of the word to be useful. YYMV.\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    # words = stem_words(words)\n",
    "    return words\n",
    "\n",
    "normalized = tokens.map(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:17.877393Z",
     "start_time": "2019-08-08T23:47:38.646358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizing verbs: changing verbs to infinite form \n",
    "\n",
    "lemmatized = normalized.map(lambda x: lemmatize_verbs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:28.358257Z",
     "start_time": "2019-08-08T23:48:17.880973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gets rid of numbers. Have to do this because vectorizer cannot lower case numbers.\n",
    "\n",
    "no_numbers = lemmatized.map(lambda x: replace_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:29.351035Z",
     "start_time": "2019-08-08T23:48:28.358257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['remix', 'explore', 'paint', 'place', 'digita...\n",
       "1    ['photo', 'grant', 'hazel', 'eat', 'cake', 'ba...\n",
       "2    ['minecraft', 'digital', 'artanimations', 'nee...\n",
       "3    ['sacramento', 'nature', 'photograph', 'series...\n",
       "4    ['north', 'africa', 'art', 'project', 'dream',...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changes all words to strings\n",
    "\n",
    "cleaned = no_numbers.astype(str)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:39.260405Z",
     "start_time": "2019-08-08T23:48:29.354027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gets rid of rare words. X is the final matrix used for machine learning.\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=.001) \n",
    "X = vectorizer.fit_transform(cleaned)\n",
    "y = kickstarter['binary_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:39.274493Z",
     "start_time": "2019-08-08T23:48:39.260405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a list of alphabetized dictionary keys to use as list features.\n",
    "\n",
    "len(list(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:42.409095Z",
     "start_time": "2019-08-08T23:48:39.278481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saves matrix for machine learning in other notebook. Comment is to read in matrix.\n",
    "\n",
    "sparse.save_npz(\"kickstarter.npz\", X)\n",
    "# your_matrix_back = sparse.load_npz(\"kickstarter.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:44.198911Z",
     "start_time": "2019-08-08T23:48:42.409095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binarizing labels into yes/no for successful/failed\n",
    "\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "y = lb.fit_transform(kickstarter.binary_state).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:44.205855Z",
     "start_time": "2019-08-08T23:48:44.200868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Minmax scaler for Naive Bayes\n",
    "\n",
    "def maxabsscaler(X):\n",
    "    ''' Performs minmaxscaling for Naive Bayes. Returns transformed matrix. '''\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    scaler = MaxAbsScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:44.217824Z",
     "start_time": "2019-08-08T23:48:44.207850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to score different machine learning models.\n",
    "\n",
    "def score_model(clf, X_input_data, y_true):\n",
    "    '''\n",
    "    Scores a machine learning model, provides summary statistics, plots ROC curve.\n",
    "   \n",
    "    Inputs: classifier, X_input_data (X_train or X_test), y_true (y_train or y_test)\n",
    "    ''' \n",
    "    \n",
    "    # Scoring models. Test model. Change to allow prediction for X_train\n",
    "    y_pred = clf.predict(X_input_data)\n",
    "\n",
    "    # Fit an accuracy score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print('\\n Accuracy Score: \\n \\n', \"{:.2%}\".format(accuracy_score(y_true, y_pred, normalize=True)))\n",
    "\n",
    "    # Create confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('\\n Confusion Matrix: \\n \\n', confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    print('\\n Classification Report: \\n \\n', classification_report(y_true, y_pred))\n",
    "\n",
    "    # Generate ROC plot\n",
    "    from sklearn.metrics import roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Compute predicted probabilities: y_pred_prob. Try except for SGD: hinge loss model.\n",
    "    try:\n",
    "        y_pred_prob = clf.predict_proba(X_input_data)[:,1]\n",
    "        \n",
    "        # Generate ROC curve values: fpr, tpr, thresholds\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        f, ax = plt.subplots(figsize=(8, 8))\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.show()\n",
    "    \n",
    "    except AttributeError:\n",
    "        print('Probability estimates not available \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:44.415183Z",
     "start_time": "2019-08-08T23:48:44.219818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting data into training and testing. In function to call before naive bayes.\n",
    "\n",
    "def train_test_split(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:48:46.617024Z",
     "start_time": "2019-08-08T23:48:44.415183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy Score: \n",
      " \n",
      " 66.50%\n",
      "\n",
      " Confusion Matrix: \n",
      " \n",
      " [[149293  23178]\n",
      " [ 73553  42704]]\n",
      "\n",
      " Classification Report: \n",
      " \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76    172471\n",
      "           1       0.65      0.37      0.47    116257\n",
      "\n",
      "    accuracy                           0.66    288728\n",
      "   macro avg       0.66      0.62      0.61    288728\n",
      "weighted avg       0.66      0.66      0.64    288728\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_minmax = maxabsscaler(X)\n",
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm = train_test_split(X_minmax, y)\n",
    "nb = MultinomialNB()\n",
    "fitted = nb.fit(X_train_mm, y_train_mm.ravel())\n",
    "score_model(fitted, X_train_mm, y_train_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:49:27.298056Z",
     "start_time": "2019-08-08T23:49:24.688818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Probability | Word\n",
      "miniatures               0.87\n",
      "printable                0.86\n",
      "cthulhu                  0.86\n",
      "pathfinder               0.82\n",
      "titanium                 0.81\n",
      "edc                      0.80\n",
      "terrain                  0.79\n",
      "dice                     0.76\n",
      "miniature                0.76\n",
      "stretch                  0.76 \n",
      "\n",
      "            Probability | Word\n",
      "cancel                    0.02\n",
      "application               0.09\n",
      "app                       0.09\n",
      "cater                     0.11\n",
      "network                   0.12\n",
      "website                   0.12\n",
      "shooter                   0.13\n",
      "users                     0.13\n",
      "platform                  0.14\n",
      "youtube                   0.14\n"
     ]
    }
   ],
   "source": [
    "# Bonus: Get the most, least likely words in blurb / topic for approval\n",
    "\n",
    "X_words = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n",
    "x = np.eye(X_words.shape[1])\n",
    "coefs = nb.predict_proba(x)[:, 1]\n",
    "feature_list = list(X_words)\n",
    "df = pd.DataFrame(coefs, feature_list, columns=['Probability | Word'])\n",
    "df['Probability | Word'] = df['Probability | Word'].apply(lambda x: \"%.2f\" % x)\n",
    "top_words = df.sort_values('Probability | Word', ascending=False).head(10)\n",
    "bottom_words = df.sort_values('Probability | Word').head(10)\n",
    "print(top_words, '\\n')\n",
    "print(bottom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T00:48:05.965867Z",
     "start_time": "2019-08-06T00:32:55.942546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exports matrix to csv\n",
    "X_words.to_csv('kickstarter_vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
